# -*- coding: utf-8 -*-
"""Prediction_using_hf_T_Tuned_Bert.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OoxYP9mxN_ktu__6oAsZckWa9WqBgcqJ
"""

!pip install transformers
!pip install datasets #needed for loading metric

"""##Import needed library"""

from google.colab import drive
drive.mount('/content/drive')

#======================================= IMPORT NEEDED LIBRARIES =======================================
import transformers
from transformers import pipeline 
import torch
print(transformers.__version__) #print th tranformer version
from transformers import (AutoModelForSequenceClassification, DistilBertForSequenceClassification, 
                          AutoTokenizer, TrainingArguments, Trainer)   #the Trainer API is used for training the model in transormer since PyTorch does not provide a training loop
from datasets import load_metric
from time import time

import pandas as pd
import numpy as np
import random

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

#======================================= NEEDED FUNCTIONS =======================================
class feedbkDataset(torch.utils.data.Dataset):    # Turn the labels and encodings into a Dataset object (using pytorch). 
    r"""PyTorch Dataset class for tuning the labels and sequence
     into a Dataset object (using pytorch). .
    Arguments:
    path (:obj:`str`):
        Path to the data partition.
    
    tokenizer (:obj:`transformers.tokenization_?`):
        Transformer type tokenizer used to process raw text into numbers.
    sequences (:obj:`list`):
        list or numpy array of the texts to be predicted.
    labels (:obj:`[int]`, `optional`)
        labels names and Values for the dataset if available.
    """
  
    def __init__(self, tokenizer, sequences, labels):
        self.encodings = tokenizer(sequences, truncation=True, padding=True, add_special_tokens=True,)
        self.labels = labels

    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        item['labels'] = torch.tensor(self.labels[idx])      # encode the label before doing this
        return item

    def __len__(self):
        return len(self.labels)


from sklearn.metrics import precision_recall_fscore_support, accuracy_score

#compute metric
def compute_metrics(pred):
    # The function that will be used to compute metrics at evaluation. Must take a
    # ~transformers.EvalPrediction and return a dictionary string to metric values.

    labels = pred.label_ids
    preds = pred.predictions.argmax(-1)
    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')  # set average to 'weighted' (to take label imbalance into account.) | 'macro' (to not take label imbalance into account)
    acc = accuracy_score(labels, preds)
    return {
        'accuracy': acc,
        'f1': f1,
        'precision': precision,
        'recall': recall
    }

def bert_prediction (pretrained_model_name_or_path, filepath, feedback_col:str, target_col:str=None, no_rows=None,
                     label_set = ['Access', 'Care received', 'Communication', "Couldn't be improved", 'Dignity', 
                        'Environment/ facilities', 'Miscellaneous', 'Staff', 'Transition/coordination']):
    r"""PyTorch Dataset class for tuning the labels and sequence
    into a Dataset object (using pytorch). .
    Arguments:
    path (:obj:`str`):
        Path to the data partition.
    
    tokenizer (:obj:`transformers.tokenization_?`):
        Transformer type tokenizer used to process raw text into numbers.
    sequences (:obj:`list`):
        list or numpy array of the texts to be predicted.
    labels (:obj:`[int]`, `optional`)
        labels names and Values for the dataset if available.
    """

    """#Load Data"""
    # Choose to read CSV from local directory or as pandas dataframe or as list
    if isinstance(filepath, str) | isinstance(filepath, pd.DataFrame):
      df = pd.read_csv(filepath, usecols=['feedback', 'label'], encoding='utf-8') if isinstance(filepath, str) else filepath      
      """# Preprocess the data for prediction"""   
      df = df.rename(columns={feedback_col: 'feedback', target_col: 'label'})     
      df.dropna(inplace=True)                                # drop rows with missing values
      df = df if no_rows == None else df.sample(no_rows)     # if number of row is not specified, use all the data
      sequences = list(df.feedback)          
      labels = np.random.randint(0, 8, len(sequences))  if target_col==None else list(df.label)
        
    elif isinstance(filepath, list):
      sequences = filepath if no_rows == None else random.sample(filepath, no_rows)      # Nt working ================================
      labels = np.random.randint(0, 8, len(sequences))      # generate dummy labels

    else: 
      raise TypeError('filepath can only be a .csv filepath (Ensure you specify the filepath correctly) or a pandas dataframe or a list!') 
      
    if not isinstance(filepath, list):
       print(f'Original Label:\n{df.head(10)} \n\nno. of feedbacks: {len(df)} \n')

    # Load the model and its tokenizer 
    model = AutoModelForSequenceClassification.from_pretrained(pretrained_model_name_or_path)
    tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path)

    # Encode the label
    if target_col != None:
      le = LabelEncoder().fit(label_set)
      df['label'] = le.transform(df.label)
      labels = list(df.label)

    if not isinstance(filepath, list):
      print(f'Mapped Label:\n{df.head(10)} \n\nno. of feedbacks: {len(df)} \n')

    # Tokenize data and convert to Dataset object
    tokenize_dataset = feedbkDataset(tokenizer, sequences, labels)  
     
    
    # instantiate a trainer object for the predition
    trainer = Trainer(
        model=model,                         
        compute_metrics=None if target_col==None else compute_metrics,   # to be used if the data contain labels
    )

    # Make the prediction 
    pred_obj = trainer.predict(tokenize_dataset)
    logits = pred_obj[0]  # get the prediction logits
    predictions = np.argmax(logits, axis=-1)
    df_eval_metrics = pd.DataFrame.from_dict(pred_obj[2], orient='index') # get the evaluation metrics (if the Dataset contains a label)
    None if target_col==None else print(df_eval_metrics) 
    return predictions

"""# Make Prediction"""

##################################### RUN ON UNSEEN TEST - new MODEL (head out ) #####################################
url = 'https://raw.githubusercontent.com/CDU-data-science-team/pxtextmining/main/datasets/text_data.csv'
data = pd.read_csv(url, usecols=['feedback', 'label'],  encoding='utf-8')
test = data.loc[8999:,:].reset_index(drop=True)                             #***TESTING****
pretrained_model_name_or_path = '/content/drive/MyDrive/Colab Notebooks/model' # the folder directory the pretrained model is saved
_ = bert_prediction (pretrained_model_name_or_path, test, 'feedback', 'label')