{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hf_T_training Bert.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CDU-data-science-team/zero-shot/blob/main/hf_T_training_Bert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hyYQ_G6QDyYA"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XzBBylSsf3YG"
      },
      "source": [
        "##Import needed library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HisumoUbEXTS",
        "outputId": "c31058dc-54ce-4a9e-dd7b-effb4f6a1c99"
      },
      "source": [
        "import transformers\n",
        "import torch\n",
        "print(transformers.__version__) #print the tranformer version\n",
        "from transformers import AutoModelForSequenceClassification, DistilBertForSequenceClassification, AutoTokenizer, TrainingArguments, Trainer   #the API for training model in transormer since PyTorch does not provide a training loop\n",
        "# from time import time\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.11.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kxRvqcywaruv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67704aea-abd6-42de-c5a1-6abea5e51aae"
      },
      "source": [
        "# # Needed to mount google drive on Colab platform\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# # To used code to  upload files on Colab platform\n",
        "# # from google.colab import files\n",
        "# # files.upload()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "espAd01Yfinb"
      },
      "source": [
        "##Loading and split the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9dnMuaHZVxyR"
      },
      "source": [
        "url = 'https://raw.githubusercontent.com/CDU-data-science-team/pxtextmining/main/datasets/text_data.csv'\n",
        "data = pd.read_csv(url, usecols=['feedback', 'label'],  encoding='utf-8')#, nrows=100)\n",
        "print(f'{data.head()} \\n\\nno. of feedbacks: {len(data)} \\n')\n",
        "df = data.loc[:8999,:]         # the remaining data is USED FOR TESTING final prediction\n",
        "\n",
        "#  take a sample to easily test pipeline\n",
        "# df = df.sample(1000).reset_index(drop=True)\n",
        "\n",
        "#fill missing value has this causes runtime error while fiting the model \n",
        "print(f'Missing values\\n{df.isna().sum()}')\n",
        "df['feedback'].fillna('Nothing', inplace = True)\n",
        "print(df.isna().sum())\n",
        "print(f'{df.head()} \\n\\nno. of feedbacks: {len(df)} \\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJw0w005-PUw"
      },
      "source": [
        "# Needed Function and Classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jOfOA11BdGN2"
      },
      "source": [
        "# Turn the labels and encodings into a Dataset object (using pytorch). \n",
        "class feedbkDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "# This will allow us to feed batches of sequences into the model at the same time)\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])      # encode the label before doing this\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DAGHYgMxEj2t"
      },
      "source": [
        "### Define a function for evaluating the model\n",
        "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
        "\n",
        "#'weighted' produced higher metrics than 'macro'\n",
        "def compute_metrics(pred):\n",
        "    r\"\"\"The function that will be used to compute metrics at evaluation. Must take a\n",
        "    `transformers.EvalPrediction` and return a dictionary string to metric values\n",
        "    Arguments:\n",
        "    pred (:obj:`transformers.EvalPrediction`):\n",
        "        model prediction from `transformers.EvalPrediction`\n",
        "    Returns:\n",
        "    :obj:`dict`: \n",
        "        dictionary string to metric values\n",
        "    \"\"\" \n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions.argmax(-1)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')  # set average to 'weighted' (to take label imbalance into account.) | 'macro' (to not take label imbalance into account)\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    return {\n",
        "        'accuracy': acc,\n",
        "        'f1': f1,\n",
        "        'precision': precision,\n",
        "        'recall': recall\n",
        "    }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QCSgjWyVjweH"
      },
      "source": [
        "#Preprocessing the data\n",
        "\n",
        "1. Encode the label\n",
        "\n",
        "2. tokenize the text feature\n",
        "\n",
        "3. Combine the label and text together and convert them into a Dataset object\n",
        "  (`torch.utils.data.Dataset` or `torch.utils.data.IterableDataset` object)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Rd_y3B34yRk"
      },
      "source": [
        "# # Needed to map label2id and id2label in the model configuration (Very useful while using model in Zeroshot or test classification pipeline)\n",
        "label2id = {}\n",
        "id2label = {}\n",
        "for v, k in enumerate(sorted( df.label.unique())):\n",
        "  label2id[k] = v\n",
        "  id2label[v] = k"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x0eZR1j6nWu2"
      },
      "source": [
        "### Define the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QkiDOjHA_yV_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "772b5da9-a3bd-424a-ba9f-472227ab67d0"
      },
      "source": [
        "# Define model_checkpoint \n",
        "model_checkpoint = \"distilbert-base-uncased\"\n",
        "# model_checkpoint = 'valhalla/distilbart-mnli-12-9'\n",
        "\n",
        "# Define the Tokenizer the test feauture \n",
        "# tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, num_labels=len(set(df.label)))\n",
        "\n",
        "# Define the model\n",
        "# model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint)\n",
        "# model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=len(set(df.label)))\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, id2label=id2label, label2id = label2id, num_labels=len(set(df.label)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias']\n",
            "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.weight', 'classifier.bias', 'pre_classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MhX6WYQYyuXJ",
        "outputId": "d27dc66e-64af-4c6c-c025-fbdda5992488"
      },
      "source": [
        "# Encode the label and create label2id and id2label - to map label2id and id2label in the model configuration (possible to be able to use model for Zeroshot)\n",
        "df['label'] = LabelEncoder().fit_transform(df.label)\n",
        "\n",
        "# Split the dataset\n",
        "seed = 0\n",
        "train_texts, train_labels = list(df.feedback), list(df.label)\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(train_texts, train_labels, test_size=.2, random_state=seed) # create validation set\n",
        "# train_texts, test_texts, train_labels, test_labels = train_test_split(train_texts, train_labels, test_size=.2, random_state=seed)   # create validation set \n",
        "\n",
        "print(f'No of feedbacks in\\nTrain set: {len(train_texts)}\\nValidation set: {len(val_texts)}') #\\nTest set: {len(test_texts)} \n",
        "\n",
        "# (truncation=True, padding=True will ensure that all of our sequences are padded to the same length and \n",
        "# are truncated to be no longer than model’s maximum input length (512 in this case). \n",
        "train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
        "val_encodings = tokenizer(val_texts, truncation=True, padding=True)\n",
        "# test_encodings = tokenizer(test_texts, truncation=True, padding=True)\n",
        "\n",
        "train_dataset = feedbkDataset(train_encodings, train_labels)\n",
        "val_dataset = feedbkDataset(val_encodings, val_labels)\n",
        "# test_dataset = feedbkDataset(test_encodings, test_labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No of feedbacks in\n",
            "Train set: 7200\n",
            "Validation set: 1800\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSSt3l1kCvBn"
      },
      "source": [
        "The steps above prepared the datasets in the way that the trainer expected. Now all we need to do is create a model to fine-tune, define the TrainingArguments and instantiate a Trainer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QuW9Tx54nF7k"
      },
      "source": [
        "#Fine-tuning the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6lQTny7GGaS"
      },
      "source": [
        "### Instantiate a TrainingArguments \n",
        "to hold all the hyperparameters we can tune for the Trainer\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1YUOWESXrvdH"
      },
      "source": [
        "# metric_name = 'accuracy'\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='/output',             # output directory\n",
        "    num_train_epochs=2,               # total number of training epochs (2 appeared to be the best from trial)\n",
        "    per_device_train_batch_size=16,   # batch size per device during training\n",
        "    per_device_eval_batch_size=64,    # batch size for evaluation\n",
        "    learning_rate=3e-5,               # (3e-5 appeared to be the best from trial)\n",
        "    warmup_steps=500,                 # number of warmup steps for learning rate scheduler\n",
        "    weight_decay=0.01,                # strength of weight decay\n",
        "    evaluation_strategy = 'epoch',\n",
        "    eval_steps = 10,                  # useful if evaluation_strategy=\"steps\"`\n",
        "    do_eval = True,                   # to run eval on the dev set.\n",
        "    # do_train = True,                  # to train the model.\n",
        "    # label_names = list(label2id.keys()), # The list of keys in our dictionary of inputs that correspond to the labels.\n",
        "    # logging_steps=10,\n",
        "    # logging_dir='./logs',             # directory for storing logs\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dt2vqnXtJwV9"
      },
      "source": [
        "print(f'No of training Samples: {len(train_dataset)}')\n",
        "training_args.device          # return the device used by this process\n",
        "# training_args.n_gpu         # the number of GPUs used by this process"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8OlmgXjhnDm"
      },
      "source": [
        "### Instantiate a trainer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ieq_Va4ocsRR"
      },
      "source": [
        "trainer = Trainer(\n",
        "    model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
        "    args=training_args,                  # training arguments, defined above\n",
        "    train_dataset=train_dataset,         # training dataset\n",
        "    eval_dataset=val_dataset,            # evaluation dataset\n",
        "    compute_metrics=compute_metrics,     # used to compute metrics at evaluation\n",
        "    tokenizer=tokenizer,                 # to preprocess the dataset and make it easier to rerun an uninterrupted training or reuse the fine-tuned model.\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VTxWdvbqd525"
      },
      "source": [
        "### Train the model \n",
        "**(took 6hrs to train on 3,200/1,000 comments of train/validation set on CPU(RAM: 12GB))**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 643
        },
        "id": "0e1w88zmVbgn",
        "outputId": "a81b36f1-59dc-4867-ef57-af1205050aed"
      },
      "source": [
        "trainer.train()   # will use GPU as long as you have access to a GPU (i.e to run script with gpu, no addition code is needed)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running training *****\n",
            "  Num examples = 7200\n",
            "  Num Epochs = 2\n",
            "  Instantaneous batch size per device = 16\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 900\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='900' max='900' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [900/900 21:27, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.860766</td>\n",
              "      <td>0.737222</td>\n",
              "      <td>0.713945</td>\n",
              "      <td>0.715620</td>\n",
              "      <td>0.737222</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.334400</td>\n",
              "      <td>0.741012</td>\n",
              "      <td>0.761667</td>\n",
              "      <td>0.750380</td>\n",
              "      <td>0.742986</td>\n",
              "      <td>0.761667</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 1800\n",
            "  Batch size = 64\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Saving model checkpoint to /output/checkpoint-500\n",
            "Configuration saved in /output/checkpoint-500/config.json\n",
            "Model weights saved in /output/checkpoint-500/pytorch_model.bin\n",
            "tokenizer config file saved in /output/checkpoint-500/tokenizer_config.json\n",
            "Special tokens file saved in /output/checkpoint-500/special_tokens_map.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1800\n",
            "  Batch size = 64\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=900, training_loss=1.0565401543511286, metrics={'train_runtime': 1288.9808, 'train_samples_per_second': 11.172, 'train_steps_per_second': 0.698, 'total_flos': 1907768667340800.0, 'train_loss': 1.0565401543511286, 'epoch': 2.0})"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "SkOakSRScOCN",
        "outputId": "4c57e4aa-1b0c-4a98-c1f4-1096b3092b90"
      },
      "source": [
        "# %%time\n",
        "# trainer.train()   # will use GPU as long as you have access to a GPU (i.e to run script with gpu, no addition code is needed)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running training *****\n",
            "  Num examples = 5760\n",
            "  Num Epochs = 5\n",
            "  Instantaneous batch size per device = 16\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 1800\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='701' max='1800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 701/1800 17:20 < 27:15, 0.67 it/s, Epoch 1.94/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.258200</td>\n",
              "      <td>1.278319</td>\n",
              "      <td>0.594444</td>\n",
              "      <td>0.515906</td>\n",
              "      <td>0.484146</td>\n",
              "      <td>0.594444</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 1800\n",
            "  Batch size = 64\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Saving model checkpoint to /output/checkpoint-500\n",
            "Configuration saved in /output/checkpoint-500/config.json\n",
            "Model weights saved in /output/checkpoint-500/pytorch_model.bin\n",
            "tokenizer config file saved in /output/checkpoint-500/tokenizer_config.json\n",
            "Special tokens file saved in /output/checkpoint-500/special_tokens_map.json\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1800' max='1800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1800/1800 45:42, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.258200</td>\n",
              "      <td>1.278319</td>\n",
              "      <td>0.594444</td>\n",
              "      <td>0.515906</td>\n",
              "      <td>0.484146</td>\n",
              "      <td>0.594444</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.844500</td>\n",
              "      <td>0.882645</td>\n",
              "      <td>0.724444</td>\n",
              "      <td>0.699981</td>\n",
              "      <td>0.695374</td>\n",
              "      <td>0.724444</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.761400</td>\n",
              "      <td>0.792243</td>\n",
              "      <td>0.748333</td>\n",
              "      <td>0.734659</td>\n",
              "      <td>0.722686</td>\n",
              "      <td>0.748333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.763800</td>\n",
              "      <td>0.786020</td>\n",
              "      <td>0.755556</td>\n",
              "      <td>0.741524</td>\n",
              "      <td>0.730436</td>\n",
              "      <td>0.755556</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.524100</td>\n",
              "      <td>0.799447</td>\n",
              "      <td>0.755556</td>\n",
              "      <td>0.743179</td>\n",
              "      <td>0.736241</td>\n",
              "      <td>0.755556</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 1800\n",
            "  Batch size = 64\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Saving model checkpoint to /output/checkpoint-1000\n",
            "Configuration saved in /output/checkpoint-1000/config.json\n",
            "Model weights saved in /output/checkpoint-1000/pytorch_model.bin\n",
            "tokenizer config file saved in /output/checkpoint-1000/tokenizer_config.json\n",
            "Special tokens file saved in /output/checkpoint-1000/special_tokens_map.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1800\n",
            "  Batch size = 64\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1800\n",
            "  Batch size = 64\n",
            "Saving model checkpoint to /output/checkpoint-1500\n",
            "Configuration saved in /output/checkpoint-1500/config.json\n",
            "Model weights saved in /output/checkpoint-1500/pytorch_model.bin\n",
            "tokenizer config file saved in /output/checkpoint-1500/tokenizer_config.json\n",
            "Special tokens file saved in /output/checkpoint-1500/special_tokens_map.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1800\n",
            "  Batch size = 64\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 45min 24s, sys: 9.93 s, total: 45min 34s\n",
            "Wall time: 45min 43s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=1800, training_loss=0.9262501242425707, metrics={'train_runtime': 2743.7985, 'train_samples_per_second': 10.496, 'train_steps_per_second': 0.656, 'total_flos': 3815537334681600.0, 'train_loss': 0.9262501242425707, 'epoch': 5.0})"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WN-Yu3DgkLg7"
      },
      "source": [
        "### Run model evaluation (on the test data - if available)\n",
        "\n",
        "No need for this if no test data because `.evaluate()` uses self.eval_dataset by default and if compute metrics has been assigned to `Trainer` object the evaluation metrics on eval_data set will be returned anyways with train()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "id": "oxcV_r83cTXX",
        "outputId": "2ae2bd53-0f96-4d8b-9b82-1474e3c4a859"
      },
      "source": [
        "#no need for this if no test data because .evaluate() uses self.eval_dataset by default and if compute metrics has been assigned to train() the evaluation metrics on eval_data set will be returned anyways\n",
        "# trainer.evaluate(test_dataset)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 1440\n",
            "  Batch size = 64\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='23' max='23' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [23/23 00:35]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'epoch': 5.0,\n",
              " 'eval_accuracy': 0.7493055555555556,\n",
              " 'eval_f1': 0.7318775678531758,\n",
              " 'eval_loss': 0.8104530572891235,\n",
              " 'eval_precision': 0.7262888029624507,\n",
              " 'eval_recall': 0.7493055555555556,\n",
              " 'eval_runtime': 36.9351,\n",
              " 'eval_samples_per_second': 38.987,\n",
              " 'eval_steps_per_second': 0.623}"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CiUf8lJGpFv9"
      },
      "source": [
        "# Save the FineTuned Model\n",
        "Save a model and its configuration file to a directory, so that it can be re-loaded using the\n",
        "        ``:func:~transformers.PreTrainedModel.from_pretrained`` class method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kTBbTMHTb7Nd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d567b1c0-e6cb-47fe-d909-5552cd8f3a7a"
      },
      "source": [
        "save_path = '/content/drive/MyDrive/Colab Notebooks/best_model/'\n",
        "trainer.save_model(save_path) # Will save the model (and tokenizer if specified when initializing the trainer object), so you can reload it using from_pretrained()\n",
        "\n",
        "# Use below codes to save the model and tokenizer seperately\n",
        "# model.save_pretrained(save_path)   # same as above but only save the model\n",
        "# tokenizer.save_pretrained(save_path) # same as above but save only the tokenizer"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/best_model/\n",
            "Configuration saved in /content/drive/MyDrive/Colab Notebooks/best_model/config.json\n",
            "Model weights saved in /content/drive/MyDrive/Colab Notebooks/best_model/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/Colab Notebooks/best_model/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/Colab Notebooks/best_model/special_tokens_map.json\n"
          ]
        }
      ]
    }
  ]
}